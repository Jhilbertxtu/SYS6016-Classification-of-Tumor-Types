{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Tumor Samples using EM\n",
    "* Caitlin Dresibach\n",
    "* Elizabeth Homan\n",
    "* Morgan Wall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from scipy.stats import norm,bernoulli\n",
    "from scipy.optimize import minimize, show_options\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import namedtuple\n",
    "#from sklearn import mixture\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set path\n",
    "path = 'C:\\\\Users\\\\mkw5c\\\\Documents\\\\School- spring 2018\\\\Machine Learning\\\\midterm project\\\\'\n",
    "data = 'data.csv'\n",
    "labels = 'labels.csv'\n",
    "rankings = 'rferankings10.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(path + data, encoding= \"ISO-8859-1\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv(path + labels, encoding= \"ISO-8859-1\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = pd.read_table(path + rankings, encoding= \"ISO-8859-1\", low_memory=False, header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add the class labels to the gene expression data\n",
    "df1[\"Class\"] = labels[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df1.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of the top 10 genes based on the RFE results, to use for subsetting the full dataset\n",
    "ranks2 = ranks[ranks[0] == 1]\n",
    "ranks2.index.name = 'topgenes'\n",
    "ranks2.reset_index(inplace=True)\n",
    "ranks2.head()\n",
    "top_genes = ranks2['topgenes'].tolist()\n",
    "top_genes.append(20531) # keep the class column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a subset of the data including all observations, but only the top 10 genes and the class labels\n",
    "\n",
    "df3 = df2.iloc[:,top_genes]\n",
    "print(df3.shape)\n",
    "print(df3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define x and y (features and classes)\n",
    "x = df3.loc[:,:\"gene_2318\"]\n",
    "y = df3.loc[:,\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view a distribution of the class labels\n",
    "h1 = plt.hist(np.array(df2[\"Class\"]), bins=20,normed=False,histtype='stepfilled',alpha=0.8); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Create a subset of the full dataset for each tumor type (to be used for selecting a random point\n",
    "#   from each class)\n",
    "\n",
    "df_prad = df3.loc[df3[\"Class\"] == \"PRAD\", :]\n",
    "df_luad = df3.loc[df3[\"Class\"] == \"LUAD\", :]\n",
    "df_brca = df3.loc[df3[\"Class\"] == \"BRCA\", :]\n",
    "df_kirc = df3.loc[df3[\"Class\"] == \"KIRC\", :]\n",
    "df_coad = df3.loc[df3[\"Class\"] == \"COAD\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization Set Up \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic Dimensions\n",
    "\n",
    "n = 801  #number of samples\n",
    "d = 10   #number of features\n",
    "k = 5    #number of clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Means (mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose a random datapoint from each cluster to be the starting mean\n",
    "mu = []\n",
    "dfs = [df_prad, df_luad, df_brca, df_kirc, df_coad]\n",
    "for i in range(5):\n",
    "    mu_ = dfs[i].iloc[np.random.choice(range(len(dfs[i])), 1, False), :]\n",
    "    mu.append(mu_)\n",
    "\n",
    "mu_df = pd.DataFrame(columns = dfs[1].columns)\n",
    "for i in range(len(mu)):\n",
    "    mu_df = pd.concat([mu_df, mu[i]])\n",
    "mu = mu_df.iloc[:, 0:10]\n",
    "mu\n",
    "\n",
    "#rename the rows\n",
    "mu.index = [\"PRAD\", \"LUAD\", \"BRCA\", \"KIRC\", \"COAD\"]\n",
    "mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Covariance (sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the covariance matrices as identity matrices for each cluster with dimensions dxd\n",
    "sigma = [np.eye(d)] * k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Class Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize pi- probability of each class\n",
    "\n",
    "pi_ = [1./k] * k #equally probable\n",
    "pi_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Responsibility Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the responsibility function as 0\n",
    "gamma = np.zeros((n, k))\n",
    "gamma.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Probability Distribution Functions\n",
    "* PDF is the probability of the class of x given theta.  \n",
    "* mixPDF = sum over all classes [the probability of the class * distribution of the class]\n",
    "* responsibility function for each sample = PDFk/mixPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the Probability Distribution Function to be used in the E- step \n",
    "# for computing the responsibility function. \n",
    "\n",
    "PDF = lambda mu, sigma: np.linalg.det(sigma) ** -.5 ** (2*np.pi) ** (-n/2.)\\\n",
    "        * np.exp(-.5 *np.einsum('ij, ij -> i', x - mu, np.dot(np.linalg.inv(sigma), \\\n",
    "        (x-mu).T).T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Expectation Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize a loop\n",
    "\n",
    "log_likelihoods = []\n",
    "theta_learning = []\n",
    "means = []\n",
    "covar = []\n",
    "probs = []\n",
    "threshold = 0.0001\n",
    "max_iter = 10000\n",
    "counter = 0\n",
    "converged = False\n",
    "\n",
    "while not converged: \n",
    "    counter += 1 \n",
    "    for k_value in range(k):\n",
    "        \n",
    "        #-- E- Step\n",
    "        #---- calculate the responsibility function\n",
    "        gamma[:, k_value] = pi_[k_value] * PDF(mu.iloc[k_value], sigma[k_value])\n",
    "\n",
    "        #---- calculate the log likelihood\n",
    "    log_likelihood = np.sum(np.log(np.sum(gamma, axis = 1)))\n",
    "    log_likelihoods.append(log_likelihood)\n",
    "\n",
    "    #---- normalize so that the responsibility matrix is row stochastic\n",
    "    gamma = (gamma.T /np.sum(gamma, axis = 1)).T\n",
    "\n",
    "    #---- determine the number of datapoints falling into each distribution\n",
    "    N_ks = np.sum(gamma, axis = 0)\n",
    "\n",
    "    for k_value in range(k):\n",
    "        #-- M- Step\n",
    "        #---- calculate the new parameters for each Gaussian\n",
    "\n",
    "        mu.iloc[k_value] = (np.sum(gamma[:, k_value]* x.T, axis = 1).T) / N_ks[k_value]\n",
    "        x_mu = np.matrix(x- mu.iloc[k_value])\n",
    "\n",
    "        sigma[k_value] = np.array(np.dot(np.multiply(x_mu.T, gamma[:, k_value]), x_mu)/ N_ks[k_value])\n",
    "\n",
    "        pi_[k_value] = 1./ n*N_ks[k_value]\n",
    "\n",
    "    #-- track progress\n",
    "    theta0 = namedtuple('theta0', ['mu', 'sigma', 'pi_', 'log_likelihood', 'iterations'])\n",
    "    theta0.mu = mu\n",
    "    theta0.sigma = sigma\n",
    "    theta0.pi_ = pi_\n",
    "    theta0.log_likelihood = log_likelihoods\n",
    "    theta0.iterations = len(log_likelihoods)\n",
    "    \n",
    "    theta_learning.append(theta0)\n",
    "    means.append(mu)\n",
    "    covar.append(sigma)\n",
    "    probs.append(pi_)\n",
    "    \n",
    "    \n",
    "    #-- check for convergence\n",
    "    if len(log_likelihoods) < 2 : continue\n",
    "    if np.abs(log_likelihood - log_likelihoods[-2])< threshold: break\n",
    "    \n",
    "    # or reached max iterations? \n",
    "    converged = counter >= max_iter \n",
    "    \n",
    "params = namedtuple('params', ['mu', 'sigma', 'pi_', 'log_likelihoods', 'num_iters'])\n",
    "params.mu = mu\n",
    "params.sigma = sigma\n",
    "params.pi_ = pi_\n",
    "params.log_likelihoods = log_likelihoods\n",
    "params.num_iters = len(log_likelihoods)  \n",
    "        \n",
    "print(len(log_likelihoods))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Resulting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(params.mu.iloc[0:10, :])\n",
    "print(params.pi_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Distributions from EM to Predict the Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that predicts the probability of each class for each observation using the \n",
    "# distribution parameters from the EM model\n",
    "def predict(x, k):\n",
    "    p = np.linalg.det(sigma[k]) ** - 0.5 * (2 * np.pi) **\\\n",
    "        (-len(x)/2) * np.exp( -0.5 * np.dot(x - mu.iloc[k] , \\\n",
    "        np.dot(np.linalg.inv(sigma[k]) , (x - mu.iloc[k]).T)))\n",
    "    prob = pi_[k]*p\n",
    "    return(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dataframe to store the predicted probabilites for each class\n",
    "prob_df = pd.DataFrame(index = np.arange(801), columns = ['PRAD', 'LUAD', 'BRCA', 'KIRC', 'COAD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the probabilities of each class for each observation\n",
    "prob_df['PRAD'] = predict(x, 0)\n",
    "prob_df['LUAD'] = predict(x, 1)\n",
    "prob_df['BRCA'] = predict(x, 2)\n",
    "prob_df['KIRC'] = predict(x, 3)\n",
    "prob_df['COAD'] = predict(x, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the sum of all of the calculated class probabilities\n",
    "\n",
    "for i in range(len(prob_df)):\n",
    "    prob_df.loc[i, 'sum'] = sum(prob_df.iloc[i, 0:5])\n",
    "prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the probability of each class geiven theta by the sum of the probabilities to get the \n",
    "# normalized probability\n",
    "\n",
    "prob_df[\"PRAD\"] = prob_df[\"PRAD\"]/prob_df['sum']\n",
    "prob_df[\"LUAD\"] = prob_df[\"LUAD\"]/prob_df['sum']\n",
    "prob_df[\"BRCA\"] = prob_df[\"BRCA\"]/prob_df['sum']\n",
    "prob_df[\"KIRC\"] = prob_df[\"KIRC\"]/prob_df['sum']\n",
    "prob_df[\"COAD\"] = prob_df[\"COAD\"]/prob_df['sum']\n",
    "prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the class of each observation by finding the class with the highest probability for each obs.\n",
    "prob_df['pred'] = prob_df.iloc[:, 0:5].idxmax(axis = 1)\n",
    "print(prob_df['pred'].value_counts())\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize columns for the ground truth label and an indicator of whether the prediction is correct\n",
    "prob_df['groundtruth'] = y\n",
    "prob_df['correct'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the accuracy\n",
    "\n",
    "for i in range(801):\n",
    "    if prob_df.loc[i, 'pred'] == prob_df.loc[i, 'groundtruth']:\n",
    "        prob_df.loc[i, 'correct'] = 1\n",
    "    else: \n",
    "        prob_df.loc[i, 'correct'] = 0\n",
    "prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the total number of correct predictions\n",
    "\n",
    "total_correct = sum(prob_df['correct'])\n",
    "total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percentage of correct observations\n",
    "\n",
    "print('Accuracy of Expectation Maximization classifier on test set: {:.2f}'.format(total_correct/len(prob_df)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
